{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tf-idf approach to generate features\n",
    "\n",
    "### Loading all the necessary packages for the subsequant analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import utils\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in German stopwords:\n",
    "\n",
    "This list helps to avoid gathering unneccessary words and thus improves the overall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = pd.read_csv('/Users/torben/PycharmProjects/toolbox/data/stopwords/stopwords.csv', index_col=None, header=0)\n",
    "stopword_u = pd.read_csv('/Users/torben/PycharmProjects/toolbox/data/stopwords/stopwordsupper.csv', index_col=None, header=0)\n",
    "swl = [stopword,stopword_u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = pd.concat(swl)\n",
    "stopwordlist = stopword_list['words'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Generate Test and Train Data\n",
    "\n",
    "This for-loop goes through the gathered data and builds dataframes for further analysis.\n",
    "\n",
    "## Train Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = {\n",
    "    1: 'politik', 2: 'wirtschaft', 3: 'finanzen', 4: 'feuilleton', 5: 'sport', 6: 'gesellschaft', 7: 'stil', \n",
    "    8: 'technik-motor', 9: 'wissen', 10: 'reise', 11: 'beruf-chance'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "for key, value in category.items():\n",
    "    raw_faz = utils.train_tables(value)\n",
    "    frames.append(raw_faz)\n",
    "    faz_train = pd.concat(frames, axis=0, ignore_index=True)\n",
    "    \n",
    "faz_train = faz_train.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the train set in the four classes we aim to predict in the end (Politics, Sports, Economy and the rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "faz_pol = faz_train.loc[faz_train['label'] == 'politik', ['title', 'detailed']]\n",
    "faz_pol['goal_val'] = 1\n",
    "\n",
    "faz_sport = faz_train.loc[faz_train['label'] == 'sport', ['title', 'detailed']]\n",
    "faz_sport['goal_val'] = 2\n",
    "\n",
    "faz_eco = faz_train.loc[faz_train['label'] == 'wirtschaft', ['title', 'detailed']]\n",
    "faz_eco['goal_val'] = 3\n",
    "\n",
    "faz_feu = faz_train.loc[faz_train['label'] == 'feuilleton', ['title', 'detailed']]\n",
    "faz_feu['goal_val'] = 4\n",
    "\n",
    "faz_fin = faz_train.loc[faz_train['label'] == 'finanzen', ['title', 'detailed']]\n",
    "faz_fin['goal_val'] = 5\n",
    "\n",
    "faz_ges = faz_train.loc[faz_train['label'] == 'gesellschaft', ['title', 'detailed']]\n",
    "faz_ges['goal_val'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "faz_train_2 = faz_train.copy()\n",
    "\n",
    "faz_rem = faz_train_2.loc[(faz_train_2['label'].isin(['stil', 'technik-motor', 'wissen', 'reise', 'beruf-chance']), ['title', 'detailed'])]\n",
    "faz_rem['goal_val'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conc = [faz_pol,faz_sport,faz_eco,faz_feu,faz_fin,faz_ges,faz_rem]\n",
    "train = pd.concat(train_conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detailed = train.drop(columns=['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Data ['title']:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titles = train.drop(columns=['detailed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titles = train_titles.rename(index=str, columns={\"title\": \"detailed\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the two sets in order to obtain more data for the model's training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = [train_detailed,train_titles]\n",
    "training_data = pd.concat(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Assign data to the respective variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data['detailed'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = training_data['goal_val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "for key, value in category.items():\n",
    "    raw_faz = utils.test_tables(value)\n",
    "    frames.append(raw_faz)\n",
    "    faz_test = pd.concat(frames, axis=0, ignore_index=True)\n",
    "    \n",
    "faz_test = faz_test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "faz_pol_t = faz_test.loc[faz_test['label'] == 'politik', ['detailed']]\n",
    "faz_pol_t['goal_val'] = 1\n",
    "\n",
    "faz_sport_t = faz_test.loc[faz_test['label'] == 'sport', ['detailed']]\n",
    "faz_sport_t['goal_val'] = 2\n",
    "\n",
    "faz_eco_t = faz_test.loc[faz_test['label'] == 'wirtschaft', ['detailed']]\n",
    "faz_eco_t['goal_val'] = 3\n",
    "\n",
    "faz_feu_t = faz_test.loc[faz_test['label'] == 'feuilleton', ['detailed']]\n",
    "faz_feu_t['goal_val'] = 4\n",
    "\n",
    "faz_fin_t = faz_test.loc[faz_test['label'] == 'finanzen', ['detailed']]\n",
    "faz_fin_t['goal_val'] = 5\n",
    "\n",
    "faz_ges_t = faz_test.loc[faz_test['label'] == 'gesellschaft', ['detailed']]\n",
    "faz_ges_t['goal_val'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "faz_test_2 = faz_test.copy()\n",
    "\n",
    "faz_rem_t = faz_test_2.loc[(faz_test_2['label'].isin(['stil', 'technik-motor', 'wissen', 'reise', 'beruf-chance']), ['detailed'])]\n",
    "faz_rem_t['goal_val'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conc = [faz_pol_t,faz_sport_t,faz_eco_t,faz_feu_t,faz_fin_t,faz_ges_t,faz_rem_t]\n",
    "testing_data = pd.concat(test_conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Assign data to the respective variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = testing_data['detailed'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = testing_data['goal_val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Get an idea of how much articles we collected & how they are distributed over the test and trainings sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of entries - Training dataset:  6344\n",
      "# of entries - Test dataset:  563\n"
     ]
    }
   ],
   "source": [
    "print(\"# of entries - Training dataset: \", X_train.shape[0])\n",
    "print(\"# of entries - Test dataset: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset:\n",
      "1    1490\n",
      "2    1256\n",
      "3    1016\n",
      "6     826\n",
      "0     700\n",
      "4     596\n",
      "5     460\n",
      "Name: goal_val, dtype: int64\n",
      "\n",
      "\n",
      "Testset:\n",
      "0    116\n",
      "1    107\n",
      "2     91\n",
      "3     76\n",
      "6     67\n",
      "4     57\n",
      "5     49\n",
      "Name: goal_val, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Trainset:')\n",
    "print(training_data.goal_val.value_counts())\n",
    "print('\\n')\n",
    "print('Testset:')\n",
    "print(testing_data.goal_val.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build a feature vector with the help of the TF-IDF Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/toolbox/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aussen', 'ausserhalb', 'dat', 'dreissig', 'einigermassen', 'gewissermassen', 'inf', 'müsst', 'regelmässig', 'schliesslich', 'weiss', 'zb'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tf_vector = TfidfVectorizer(stop_words=stopwordlist,\n",
    "                            analyzer='word',\n",
    "                            ngram_range=(1,3),\n",
    "                            max_features=10000, \n",
    "                            use_idf=True,\n",
    "                            smooth_idf=True,\n",
    "                           )\n",
    "\n",
    "train = tf_vector.fit_transform(X_train)\n",
    "test = tf_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Apply Linear SupportVectorMachines with new feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/toolbox/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lclf = LinearSVC( \n",
    "    tol=1e-5, \n",
    "    loss='hinge')\n",
    "\n",
    "lclf.fit(train, y_train) \n",
    "pred = lclf.predict(test)\n",
    "test_accuracy = accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Build a data frame with sample texts to test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = []\n",
    "\n",
    "samples = utils.test_tables('aktuell')\n",
    "sample.append(samples)\n",
    "\n",
    "faz_sample = pd.concat(sample, axis=0, ignore_index=True)\n",
    "faz_samples = faz_sample.drop_duplicates()\n",
    "\n",
    "details = faz_samples['detailed']\n",
    "sample_text = details.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Randomly pick a text from the samples data frame and let the model predict its class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mehrere minderjährige Mädchen soll ein junger Mann im Raum Würzburg vergewaltigt und teilweise schwer sexuell missbraucht haben.In Würzburg steht ein junger Mann vor Gericht. Er soll mehrere Mädchen vergewaltigt und die Tat gefilmt haben.\n",
      "\n",
      "\n",
      "The text above belongs to the category: \"Gesellschaft\"\n"
     ]
    }
   ],
   "source": [
    "sample = random.choice(sample_text)\n",
    "print(sample)\n",
    "print('\\n')\n",
    "\n",
    "exemp = sample.lower()\n",
    "test_article = tf_vector.transform([exemp])\n",
    "prediction = lclf.predict(test_article)\n",
    "\n",
    "utils.print_classification(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Alternatively: Copy and paste your text below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Ferien in einem Euro-Land? Das scheint vielen britischen Pauschaltouristen derzeit nicht geheuer zu sein. Laut Reiseveranstalter Thomas Cook buchen sie wegen des Brexits verstärkt außerhalb der Währungsunion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ferien in einem Euro-Land? Das scheint vielen britischen Pauschaltouristen derzeit nicht geheuer zu sein. Laut Reiseveranstalter Thomas Cook buchen sie wegen des Brexits verstärkt außerhalb der Währungsunion'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text above belongs to the category: \"Politik\"\n"
     ]
    }
   ],
   "source": [
    "ex = txt.lower()\n",
    "test_art = tf_vector.transform([ex])\n",
    "pre = lclf.predict(test_art)\n",
    "\n",
    "utils.print_classification(pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voilà! Das Modell hat den Text richtig zugeordnet :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
