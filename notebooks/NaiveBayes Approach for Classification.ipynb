{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import the preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = {\n",
    "    1: 'politik', 2: 'wirtschaft', 3: 'finanzen', 4: 'feuilleton', 5: 'sport', 6: 'gesellschaft', 7: 'stil', \n",
    "    8: 'technik-motor', 9: 'wissen', 10: 'reise', 11: 'beruf-chance'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tables(category):\n",
    "    raw_train = []\n",
    "    if category is 'aktuell':\n",
    "        path = f\"../new_data/aktuell/\"\n",
    "    else:\n",
    "        path = f\"../new_data/{category}/\"\n",
    "        \n",
    "    all_files = glob.glob(path + '*.csv')\n",
    "\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        raw_train.append(df)\n",
    "    faz_articles = pd.concat(raw_train, axis=0, ignore_index=True)\n",
    "    faz_articles['label'] = category\n",
    "\n",
    "    return faz_articles\n",
    "\n",
    "def test_tables(category):\n",
    "    raw_test = []\n",
    "    if category is 'aktuell':\n",
    "        path = f\"../data/aktuell/\"\n",
    "    else:\n",
    "        path = f\"../data/{category}/\"\n",
    "        \n",
    "    all_files = glob.glob(path + '*.csv')\n",
    "\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        raw_test.append(df)\n",
    "    faz_article = pd.concat(raw_test, axis=0, ignore_index=True)\n",
    "    faz_article['label'] = category\n",
    "\n",
    "    return faz_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Load feature list from NaiveBayes approach notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.read_csv('/Users/torben/PycharmProjects/toolbox/feature/features.csv', index_col=None, header=0)\n",
    "\n",
    "feature_list = feat['words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary={}\n",
    "\n",
    "for i in range(len(feature_list)):\n",
    "    dictionary[feature_list[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "for key, value in category.items():\n",
    "    raw_faz = train_tables(value)\n",
    "    frames.append(raw_faz)\n",
    "    faz_train = pd.concat(frames, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "faz_train = faz_train.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "faz_pol = faz_train[faz_train.label == 'politik']\n",
    "faz_pol_detailed = faz_pol.drop(columns=['link', 'published','title','label'])\n",
    "faz_pol_titles = faz_pol.drop(columns=['link', 'published','detailed','label'])\n",
    "faz_pol_titles = faz_pol_titles.rename(index=str, columns={\"title\": \"detailed\"})\n",
    "faz_pol_combined = [faz_pol_detailed,faz_pol_titles]\n",
    "faz_pol_tot = pd.concat(faz_pol_combined)\n",
    "\n",
    "faz_sport = faz_train[faz_train.label == 'sport']\n",
    "faz_sport_detailed = faz_sport.drop(columns=['link', 'published','title','label'])\n",
    "faz_sport_titles = faz_sport.drop(columns=['link', 'published','detailed','label'])\n",
    "faz_sport_titles = faz_sport_titles.rename(index=str, columns={\"title\": \"detailed\"})\n",
    "faz_sport_combined = [faz_sport_detailed,faz_sport_titles]\n",
    "faz_sport_tot = pd.concat(faz_sport_combined)\n",
    "\n",
    "faz_eco = faz_train[faz_train.label == 'wirtschaft']\n",
    "faz_eco_detailed = faz_eco.drop(columns=['link', 'published','title','label'])\n",
    "faz_eco_titles = faz_eco.drop(columns=['link', 'published','detailed','label'])\n",
    "faz_eco_titles = faz_eco_titles.rename(index=str, columns={\"title\": \"detailed\"})\n",
    "faz_eco_combined = [faz_eco_detailed,faz_eco_titles]\n",
    "faz_eco_tot = pd.concat(faz_eco_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "faz_train_2 = faz_train.copy()\n",
    "\n",
    "faz_train_2 = faz_train_2[faz_train_2.label != 'politik']\n",
    "faz_train_2 = faz_train_2[faz_train_2.label != 'sport']\n",
    "faz_train_2 = faz_train_2[faz_train_2.label != 'wirtschaft']\n",
    "\n",
    "faz_rem = faz_train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "faz_rem_detailed = faz_rem.drop(columns=['link', 'published','title','label'])\n",
    "faz_rem_titles = faz_rem.drop(columns=['link', 'published','detailed','label'])\n",
    "faz_rem_titles = faz_rem_titles.rename(index=str, columns={\"title\": \"detailed\"})\n",
    "faz_rem_combined = [faz_rem_detailed,faz_rem_titles]\n",
    "faz_rem_tot = pd.concat(faz_rem_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Iterate over the respective tables of train data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over 'pol' buzzwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_buzzword = []\n",
    "\n",
    "for index, row in faz_pol_tot.iterrows():\n",
    "    new_line = np.zeros(10000)\n",
    "    words = list(row['detailed'].split(' '))\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            new_line[dictionary[word]] += 1\n",
    "    pol_buzzword.append(new_line)\n",
    "    \n",
    "    \n",
    "df_pol = pd.DataFrame(pol_buzzword, columns=feature_list)\n",
    "df_pol['goal_val'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over 'sport' buzzwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_buzzword = []\n",
    "\n",
    "for index, row in faz_sport_tot.iterrows():\n",
    "    new_line = np.zeros(10000)\n",
    "    words = list(row['detailed'].split(' '))\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            new_line[dictionary[word]] += 1\n",
    "    sport_buzzword.append(new_line)\n",
    "\n",
    "df_sport = pd.DataFrame(sport_buzzword, columns=feature_list)\n",
    "df_sport['goal_val'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over 'eco' buzzwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_buzzword = []\n",
    "\n",
    "for index, row in faz_eco_tot.iterrows():\n",
    "    new_line = np.zeros(10000)\n",
    "    words = list(row['detailed'].split(' '))\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            new_line[dictionary[word]] += 1\n",
    "    eco_buzzword.append(new_line)\n",
    "    \n",
    "df_eco = pd.DataFrame(eco_buzzword, columns=feature_list)\n",
    "df_eco['goal_val'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over 'rem' buzzwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_buzzword = []\n",
    "\n",
    "for index, row in faz_rem_tot.iterrows():\n",
    "    new_line = np.zeros(10000)\n",
    "    words = list(row['detailed'].split(' '))\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            new_line[dictionary[word]] += 1\n",
    "    rem_buzzword.append(new_line)\n",
    "    \n",
    "df_rem = pd.DataFrame(rem_buzzword, columns=feature_list)\n",
    "df_rem['goal_val'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tables = [df_pol, df_eco, df_sport, df_rem]\n",
    "train = pd.concat(train_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "for key, value in category.items():\n",
    "    raw_faz = test_tables(value)\n",
    "    frames.append(raw_faz)\n",
    "    faz_test = pd.concat(frames, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "faz_test = faz_test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "faz_pol_t = faz_test[faz_test.label == 'politik']\n",
    "faz_sport_t = faz_test[faz_test.label == 'sport']\n",
    "faz_eco_t = faz_test[faz_test.label == 'wirtschaft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "faz_test_2 = faz_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "faz_test_2 = faz_test_2[faz_test_2.label != 'politik']\n",
    "faz_test_2 = faz_test_2[faz_test_2.label != 'sport']\n",
    "faz_test_2 = faz_test_2[faz_test_2.label != 'wirtschaft']\n",
    "faz_rem_t = faz_test_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pol_t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_buzzword_t = []\n",
    "\n",
    "for index, row in faz_pol_t.iterrows():\n",
    "    new_line = np.zeros(10000)\n",
    "    words = list(row['detailed'].split(' '))\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            new_line[dictionary[word]] += 1\n",
    "    pol_buzzword_t.append(new_line)\n",
    "    \n",
    "df_pol_t = pd.DataFrame(pol_buzzword_t, columns=feature_list)\n",
    "df_pol_t['goal_val'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sport_t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_buzzword_t = []\n",
    "\n",
    "for index, row in faz_sport_t.iterrows():\n",
    "    new_line = np.zeros(10000)\n",
    "    words = list(row['detailed'].split(' '))\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            new_line[dictionary[word]] += 1\n",
    "    sport_buzzword_t.append(new_line)\n",
    "    \n",
    "df_sport_t = pd.DataFrame(sport_buzzword_t, columns=feature_list)\n",
    "df_sport_t['goal_val'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eco_t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_buzzword_t = []\n",
    "\n",
    "for index, row in faz_eco_t.iterrows():\n",
    "    new_line = np.zeros(10000)\n",
    "    words = list(row['detailed'].split(' '))\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            new_line[dictionary[word]] += 1\n",
    "    eco_buzzword_t.append(new_line)\n",
    "    \n",
    "df_eco_t = pd.DataFrame(eco_buzzword_t, columns=feature_list)\n",
    "df_eco_t['goal_val'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rem_t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_buzzword_t = []\n",
    "\n",
    "for index, row in faz_rem_t.iterrows():\n",
    "    new_line = np.zeros(10000)\n",
    "    words = list(row['detailed'].split(' '))\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            new_line[dictionary[word]] += 1\n",
    "    rem_buzzword_t.append(new_line)\n",
    "    \n",
    "df_rem_t = pd.DataFrame(rem_buzzword_t, columns=feature_list)\n",
    "df_rem_t['goal_val'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tables = [df_pol_t, df_sport_t, df_eco_t, df_rem_t]\n",
    "test = pd.concat(test_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['goal_val']\n",
    "y_test = test['goal_val']\n",
    "X_train = train.drop(['goal_val'], axis=1)\n",
    "X_test = test.drop(['goal_val'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_X_train = X_train.copy()\n",
    "hot_X_train[hot_X_train > 0] = 1\n",
    "\n",
    "hot_X_test = X_test.copy()\n",
    "hot_X_test[hot_X_test > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hot_X_train\n",
    "X_test =  hot_X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TEST prediction is 58.614564831261106 % ACCURATE\n"
     ]
    }
   ],
   "source": [
    "sgd = linear_model.SGDClassifier(\n",
    "                                loss=\"log\",\n",
    "                                max_iter=1000,\n",
    "                                tol=1e-3\n",
    "                                )\n",
    "\n",
    "sgd.fit(X_train, y_train)  \n",
    "\n",
    "prediction = sgd.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, prediction)\n",
    "\n",
    "print(f'The TEST prediction is {test_accuracy*100} % ACCURATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
